{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_data = pd.read_csv('data/labels.csv')\n",
    "\n",
    "# Read the training.csv file\n",
    "training_data = pd.read_csv('data/training.csv')\n",
    "training_data = pd.merge(training_data, labels_data, on='label')\n",
    "\n",
    "# Read the validations.csv file\n",
    "validation_data = pd.read_csv('data/validations.csv')\n",
    "validation_data = pd.merge(validation_data, labels_data, on='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>Type of Clause</th>\n",
       "      <th>Degree of Unfairness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The purpose of this website, 9gag.com (the “Si...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You agree that neither 9GAG, Inc nor the Site ...</td>\n",
       "      <td>ltd2</td>\n",
       "      <td>Limitation of Liability</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9GAG, Inc retains the right to create limits o...</td>\n",
       "      <td>ter3</td>\n",
       "      <td>Unilateral  Termination</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Site is protected by copyright as a collec...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subscriber may download or copy the Content, a...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>If you are a player outside of the United Stat...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>Any attempted notice that does not follow thes...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>You agree that given the unique and irreplacea...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>You agree to limit your claims to claims for m...</td>\n",
       "      <td>unc</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>We are not liable for any changes or problems ...</td>\n",
       "      <td>ltd1</td>\n",
       "      <td>Limitation of Liability</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2993 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content label  \\\n",
       "0     The purpose of this website, 9gag.com (the “Si...   unc   \n",
       "1     You agree that neither 9GAG, Inc nor the Site ...  ltd2   \n",
       "2     9GAG, Inc retains the right to create limits o...  ter3   \n",
       "3     The Site is protected by copyright as a collec...   unc   \n",
       "4     Subscriber may download or copy the Content, a...   unc   \n",
       "...                                                 ...   ...   \n",
       "2988  If you are a player outside of the United Stat...   unc   \n",
       "2989  Any attempted notice that does not follow thes...   unc   \n",
       "2990  You agree that given the unique and irreplacea...   unc   \n",
       "2991  You agree to limit your claims to claims for m...   unc   \n",
       "2992  We are not liable for any changes or problems ...  ltd1   \n",
       "\n",
       "                Type of Clause  Degree of Unfairness  \n",
       "0                      Unknown                     0  \n",
       "1     Limitation of Liability                      2  \n",
       "2     Unilateral  Termination                      3  \n",
       "3                      Unknown                     0  \n",
       "4                      Unknown                     0  \n",
       "...                        ...                   ...  \n",
       "2988                   Unknown                     0  \n",
       "2989                   Unknown                     0  \n",
       "2990                   Unknown                     0  \n",
       "2991                   Unknown                     0  \n",
       "2992  Limitation of Liability                      1  \n",
       "\n",
       "[2993 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "X_train = training_data['content']\n",
    "y_train = training_data['label']\n",
    "\n",
    "# Prepare the validation data\n",
    "X_val = validation_data['content']\n",
    "y_val = validation_data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546739984289081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the fitted vectorizer\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict labels for the validation data\n",
    "y_pred = model.predict(X_val_vec)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the model using Hyper Parameter Tuning\n",
    "\n",
    "todo: write exlanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyankamarwaha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyankamarwaha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/priyankamarwaha/anaconda3/envs/gdpr_project/lib/python3.11/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.8986645718774549\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.base import TransformerMixin\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Custom text preprocessor\n",
    "class TextPreprocessor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # No need to initialize lemmatizer or stopwords here\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Initialize the lemmatizer and stopwords inside the method\n",
    "        from nltk.corpus import stopwords\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        cleaned_docs = []\n",
    "        for doc in X:\n",
    "            doc = doc.lower()\n",
    "            doc = re.sub(r'\\W', ' ', doc)\n",
    "            doc = re.sub(r'\\s+', ' ', doc)\n",
    "            doc = ' '.join([lemmatizer.lemmatize(token) for token in doc.split() if token not in english_stopwords])\n",
    "            cleaned_docs.append(doc)\n",
    "        return cleaned_docs\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # No changes needed here\n",
    "        return self\n",
    "\n",
    "# Pipeline with text preprocessing, TF-IDF vectorization, and an SVM model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Parameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'vectorizer__max_features': [5000, 10000],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Grid search with n_jobs set to 1 to avoid multiprocessing issues in notebooks\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict labels for the validation data\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Best Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning based baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: Write explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankamarwaha/anaconda3/envs/gdpr_project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/Users/priyankamarwaha/anaconda3/envs/gdpr_project/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from baseline_dl_model import GdprClassifier, create_data_loader, ExtendedLabelEncoder\n",
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Encode labels with the ExtendedLabelEncoder\n",
    "extended_le = ExtendedLabelEncoder()\n",
    "y_train_enc = extended_le.fit_transform(y_train)\n",
    "y_val_enc = extended_le.transform(y_val)\n",
    "\n",
    "# Adjust this based on the number of unique labels + 1 for 'unknown'\n",
    "NUM_CLASSES = len(extended_le.classes_)\n",
    "\n",
    "# Data loaders\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256\n",
    "train_data_loader = create_data_loader(X_train, y_train_enc, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(X_val, y_val_enc, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = GdprClassifier(n_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/188 [00:00<?, ?it/s]/Users/priyankamarwaha/anaconda3/envs/gdpr_project/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_data_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # TODO: Remove break statement\n",
    "        break\n",
    "\n",
    "        progress_bar.set_postfix({'Training Loss': loss.item()})\n",
    "    # TODO: Remove\n",
    "    break\n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Training Loss: {avg_train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .baseline_dl_model import accuracy_score\n",
    "\n",
    "accuracy_score(model, val_data_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdpr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
